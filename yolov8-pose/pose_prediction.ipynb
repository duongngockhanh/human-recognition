{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNCNRHFuSGrqkqTQo2wGHdJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"zyU-oz3QZ7fs"},"outputs":[],"source":["!pip install ultralytics"]},{"cell_type":"code","source":["from ultralytics import YOLO\n","\n","model = YOLO('yolov8n-pose.pt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-FszqvuJa6gd","executionInfo":{"status":"ok","timestamp":1701345152725,"user_tz":-420,"elapsed":7803,"user":{"displayName":"Khanh Duong","userId":"02272471262918357102"}},"outputId":"4ef234b3-1483-47b4-a47a-3c0046b44712"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n-pose.pt to 'yolov8n-pose.pt'...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 6.51M/6.51M [00:00<00:00, 252MB/s]\n"]}]},{"cell_type":"code","source":["results = model(\"a_standing_image.jpg\")\n","\n","for result in results:\n","  keypoints = result.keypoints\n","  print(keypoints)"],"metadata":{"id":"-sMTB7CHa-be","executionInfo":{"status":"ok","timestamp":1701345386722,"user_tz":-420,"elapsed":2589,"user":{"displayName":"Khanh Duong","userId":"02272471262918357102"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b1bbae8e-ae89-41aa-c4c4-a0f63f393d83"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","image 1/1 /content/a_standing_image.jpg: 640x448 1 person, 310.0ms\n","Speed: 16.7ms preprocess, 310.0ms inference, 29.9ms postprocess per image at shape (1, 3, 640, 448)\n","ultralytics.engine.results.Keypoints object with attributes:\n","\n","conf: tensor([[0.9930, 0.9696, 0.9733, 0.7523, 0.7987, 0.9963, 0.9979, 0.9864, 0.9919, 0.9834, 0.9897, 0.9995, 0.9996, 0.9986, 0.9990, 0.9777, 0.9826]])\n","data: tensor([[[121.1554,  44.0426,   0.9930],\n","         [126.5637,  37.7366,   0.9696],\n","         [115.0803,  38.8982,   0.9733],\n","         [134.9856,  42.8541,   0.7523],\n","         [107.5249,  45.2500,   0.7987],\n","         [150.1250,  83.7632,   0.9963],\n","         [ 92.8896,  85.8184,   0.9979],\n","         [164.0836, 132.6036,   0.9864],\n","         [ 76.5510, 134.7271,   0.9919],\n","         [166.1864, 171.4431,   0.9834],\n","         [ 99.1378, 169.1646,   0.9897],\n","         [147.4086, 179.9561,   0.9995],\n","         [109.0900, 182.5566,   0.9996],\n","         [156.3716, 248.3040,   0.9986],\n","         [110.3833, 252.3112,   0.9990],\n","         [163.9946, 315.1884,   0.9777],\n","         [113.8001, 318.3051,   0.9826]]])\n","has_visible: True\n","orig_shape: (360, 252)\n","shape: torch.Size([1, 17, 3])\n","xy: tensor([[[121.1554,  44.0426],\n","         [126.5637,  37.7366],\n","         [115.0803,  38.8982],\n","         [134.9856,  42.8541],\n","         [107.5249,  45.2500],\n","         [150.1250,  83.7632],\n","         [ 92.8896,  85.8184],\n","         [164.0836, 132.6036],\n","         [ 76.5510, 134.7271],\n","         [166.1864, 171.4431],\n","         [ 99.1378, 169.1646],\n","         [147.4086, 179.9561],\n","         [109.0900, 182.5566],\n","         [156.3716, 248.3040],\n","         [110.3833, 252.3112],\n","         [163.9946, 315.1884],\n","         [113.8001, 318.3051]]])\n","xyn: tensor([[[0.4808, 0.1223],\n","         [0.5022, 0.1048],\n","         [0.4567, 0.1081],\n","         [0.5357, 0.1190],\n","         [0.4267, 0.1257],\n","         [0.5957, 0.2327],\n","         [0.3686, 0.2384],\n","         [0.6511, 0.3683],\n","         [0.3038, 0.3742],\n","         [0.6595, 0.4762],\n","         [0.3934, 0.4699],\n","         [0.5850, 0.4999],\n","         [0.4329, 0.5071],\n","         [0.6205, 0.6897],\n","         [0.4380, 0.7009],\n","         [0.6508, 0.8755],\n","         [0.4516, 0.8842]]])\n"]}]},{"cell_type":"code","source":["model.predict('a_standing_image.jpg', save=True, imgsz=320, conf=0.5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fjrLbvfObBUq","executionInfo":{"status":"ok","timestamp":1701345390552,"user_tz":-420,"elapsed":307,"user":{"displayName":"Khanh Duong","userId":"02272471262918357102"}},"outputId":"59e844d8-9a35-4028-8cbc-d0df29f81c8e"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","image 1/1 /content/a_standing_image.jpg: 320x224 1 person, 56.1ms\n","Speed: 1.2ms preprocess, 56.1ms inference, 0.9ms postprocess per image at shape (1, 3, 320, 224)\n","Results saved to \u001b[1mruns/pose/predict\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["[ultralytics.engine.results.Results object with attributes:\n"," \n"," boxes: ultralytics.engine.results.Boxes object\n"," keypoints: ultralytics.engine.results.Keypoints object\n"," masks: None\n"," names: {0: 'person'}\n"," orig_img: array([[[255, 255, 255],\n","         [255, 255, 255],\n","         [255, 255, 255],\n","         ...,\n","         [255, 255, 255],\n","         [255, 255, 255],\n","         [255, 255, 255]],\n"," \n","        [[255, 255, 255],\n","         [255, 255, 255],\n","         [255, 255, 255],\n","         ...,\n","         [255, 255, 255],\n","         [255, 255, 255],\n","         [255, 255, 255]],\n"," \n","        [[255, 255, 255],\n","         [255, 255, 255],\n","         [255, 255, 255],\n","         ...,\n","         [255, 255, 255],\n","         [255, 255, 255],\n","         [255, 255, 255]],\n"," \n","        ...,\n"," \n","        [[255, 255, 255],\n","         [255, 255, 255],\n","         [255, 255, 255],\n","         ...,\n","         [255, 255, 255],\n","         [255, 255, 255],\n","         [255, 255, 255]],\n"," \n","        [[255, 255, 255],\n","         [255, 255, 255],\n","         [255, 255, 255],\n","         ...,\n","         [255, 255, 255],\n","         [255, 255, 255],\n","         [255, 255, 255]],\n"," \n","        [[255, 255, 255],\n","         [255, 255, 255],\n","         [255, 255, 255],\n","         ...,\n","         [255, 255, 255],\n","         [255, 255, 255],\n","         [255, 255, 255]]], dtype=uint8)\n"," orig_shape: (360, 252)\n"," path: '/content/a_standing_image.jpg'\n"," probs: None\n"," save_dir: 'runs/pose/predict'\n"," speed: {'preprocess': 1.16729736328125, 'inference': 56.14805221557617, 'postprocess': 0.9148120880126953}]"]},"metadata":{},"execution_count":4}]}]}